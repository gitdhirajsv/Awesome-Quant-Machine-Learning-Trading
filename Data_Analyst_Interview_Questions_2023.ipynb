{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Data Analyst Interview Questions 2023",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitdhirajsv/Awesome-Quant-Machine-Learning-Trading/blob/master/Data_Analyst_Interview_Questions_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Data Analyst.jpg](attachment:d7576614-430f-44db-abe5-a751f03de681.jpg)"
      ],
      "metadata": {
        "id": "JLA5cvI-ACN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Python**"
      ],
      "metadata": {
        "id": "NhZ0W9szACN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be stated that a significant portion, approximately 80%, of the responsibilities of a Machine Learning Engineer and Data Analyst revolves around data acquisition and data cleaning. Pandas, being one of the fundamental libraries in any data science workflow, plays a crucial role in facilitating data manipulation, transformation, and manipulation. Let's explore and review the frequently encountered Pandas and Python Interview Questions and Answers that are essential for your upcoming machine learning, data analyst, or data science interview.\n",
        "\n",
        "**Questions will be from easy to difficult**\n",
        "\n",
        "1. **Amazon Questions**\n",
        "2. **Frobes Questions**\n",
        "3. **Flipkart Questions**\n",
        "4. **Meta/facebook Questions**\n",
        "5. **Lyft Questions**"
      ],
      "metadata": {
        "id": "Y5PTw-adACN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. How to create new columns derived from existing columns in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "* We create a new column by assigning the output to the DataFrame with a new column name in between the [].\n",
        "* Let's say we want to create a new column 'C' whose values are the multiplication of column 'B' with column 'A'. The operation will be easy to implement and will be element-wise, so there's no need to loop over rows."
      ],
      "metadata": {
        "id": "VPQE2oo7ACN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create example data\n",
        "df = pd.DataFrame({\n",
        "  \"A\": [420, 380, 390,450,320,890,100,200],\n",
        "  \"B\": [50, 40, 45,92,40,50,76,98]\n",
        "})\n",
        "\n",
        "df[\"C\"] = df[\"A\"] * df[\"B\"]\n",
        "\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.259429Z",
          "iopub.execute_input": "2023-07-11T11:07:59.259859Z",
          "iopub.status.idle": "2023-07-11T11:07:59.338486Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.25982Z",
          "shell.execute_reply": "2023-07-11T11:07:59.337298Z"
        },
        "trusted": true,
        "id": "ERhD1BHtACN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.A column in a df has boolean True/False values, but for further calculations, we need 1/0 representation. How would you transform it?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "A succinct way to convert a single column of boolean values to a column of integers 1 or 0 is:"
      ],
      "metadata": {
        "id": "K47lk0ASACN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"A\"] = df[\"A\"].astype(int)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.34055Z",
          "iopub.execute_input": "2023-07-11T11:07:59.340907Z",
          "iopub.status.idle": "2023-07-11T11:07:59.347208Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.340876Z",
          "shell.execute_reply": "2023-07-11T11:07:59.345953Z"
        },
        "trusted": true,
        "id": "HzXS6oHhACN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3 Describe how you will get the names of columns of a DataFrame in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "By Simply iterating over columns, and printing the values."
      ],
      "metadata": {
        "id": "1vf1Dy93ACN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "    print(col)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.348754Z",
          "iopub.execute_input": "2023-07-11T11:07:59.349775Z",
          "iopub.status.idle": "2023-07-11T11:07:59.363357Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.349742Z",
          "shell.execute_reply": "2023-07-11T11:07:59.362453Z"
        },
        "trusted": true,
        "id": "q0HsRAaGACN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(df.columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.364941Z",
          "iopub.execute_input": "2023-07-11T11:07:59.365477Z",
          "iopub.status.idle": "2023-07-11T11:07:59.377795Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.365443Z",
          "shell.execute_reply": "2023-07-11T11:07:59.376414Z"
        },
        "trusted": true,
        "id": "IpHUkGM3ACN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the column.values() method to return an array of index.\n",
        "\n",
        "list(df.columns.values)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.392084Z",
          "iopub.execute_input": "2023-07-11T11:07:59.392596Z",
          "iopub.status.idle": "2023-07-11T11:07:59.40029Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.392516Z",
          "shell.execute_reply": "2023-07-11T11:07:59.39914Z"
        },
        "trusted": true,
        "id": "xCAm1eDsACN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using sorted() method, which will return the list of columns sorted in alphabetical order.\n",
        "\n",
        "sorted(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.424268Z",
          "iopub.execute_input": "2023-07-11T11:07:59.425064Z",
          "iopub.status.idle": "2023-07-11T11:07:59.432773Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.425018Z",
          "shell.execute_reply": "2023-07-11T11:07:59.431541Z"
        },
        "trusted": true,
        "id": "NUH3GiNKACN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q4: How are iloc() and loc() different?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "* `DataFrame.iloc` is a method used to retrieve data from a Data frame, and it is an integer position-based locator (from 0 to length-1 of the axis), but may also be used with a boolean array. It takes input as integer, arrays of integers, a slice object, boolean array and functions.\n"
      ],
      "metadata": {
        "id": "Di36qTPJACN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]\n",
        "df.iloc[-5:]\n",
        "df.iloc[:, 2]    # the : in the first position indicates all rows\n",
        "df.iloc[:3, :3] # The upper-left 3 X 3 entries (assuming df has 3+ rows and columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.465895Z",
          "iopub.execute_input": "2023-07-11T11:07:59.467686Z",
          "iopub.status.idle": "2023-07-11T11:07:59.483301Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.467633Z",
          "shell.execute_reply": "2023-07-11T11:07:59.4821Z"
        },
        "trusted": true,
        "id": "KlokBJxTACN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `DataFrame.loc` gets rows (and/or columns) with particular labels. It takes input as a single label, list of arrays and slice objects with labels.\n"
      ],
      "metadata": {
        "id": "JOtLMBVIACN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'Name': ['John', 'Emily', 'Michael', 'Emma', 'Daniel'],\n",
        "    'Age': [25, 30, 35, 28, 32],\n",
        "    'City': ['New York', 'London', 'Paris', 'Sydney', 'Tokyo'],\n",
        "    'Salary': [50000, 60000, 70000, 55000, 65000]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(df, '\\n \\n')\n",
        "print(df.loc[0],'#Index + 1st column','\\n')\n",
        "print(df.loc[1:3],'# 0 to 2nd rows ','\\n')\n",
        "print(df.loc[:, 'Name'],'# All rows and only Name column')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.495165Z",
          "iopub.execute_input": "2023-07-11T11:07:59.496152Z",
          "iopub.status.idle": "2023-07-11T11:07:59.513466Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.496113Z",
          "shell.execute_reply": "2023-07-11T11:07:59.512313Z"
        },
        "trusted": true,
        "id": "yALfepnUACN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.526266Z",
          "iopub.execute_input": "2023-07-11T11:07:59.526987Z",
          "iopub.status.idle": "2023-07-11T11:07:59.535394Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.526926Z",
          "shell.execute_reply": "2023-07-11T11:07:59.534447Z"
        },
        "trusted": true,
        "id": "rlHD3FByACN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q5: How can you sort the DataFrame?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "The function used for sorting in pandas is called DataFrame.sort_values(). It is used to sort a DataFrame by its column or row values. The function comes with a lot of parameters, but the most important ones to consider for sort are:\n",
        "\n",
        "* **by:** The optional by parameter is used to specify the column/row(s) which are used to determine the sorted order.\n",
        "* **axis:** specifies whether sort for row (0) or columns (1),\n",
        "* **ascending:** specifies whether to sort the dataframe in ascending or descending order. The default value is ascending. To sort in descending order, we need to specify ascending=False."
      ],
      "metadata": {
        "id": "ke6KGfjTACN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df = pd.DataFrame({\n",
        "    'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n",
        "    'col2': [2, 1, 9, 8, 7, 4],\n",
        "    'col3': [0, 1, 9, 4, 2, 3],\n",
        "    'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n",
        "})\n",
        "print(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.555602Z",
          "iopub.execute_input": "2023-07-11T11:07:59.556646Z",
          "iopub.status.idle": "2023-07-11T11:07:59.56778Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.556597Z",
          "shell.execute_reply": "2023-07-11T11:07:59.566799Z"
        },
        "trusted": true,
        "id": "9Zd_J3vXACN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by = ['col1'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.585165Z",
          "iopub.execute_input": "2023-07-11T11:07:59.585817Z",
          "iopub.status.idle": "2023-07-11T11:07:59.598272Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.585779Z",
          "shell.execute_reply": "2023-07-11T11:07:59.597456Z"
        },
        "trusted": true,
        "id": "oUZpeSV-ACN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by = ['col1', 'col2'], ascending = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.675432Z",
          "iopub.execute_input": "2023-07-11T11:07:59.676227Z",
          "iopub.status.idle": "2023-07-11T11:07:59.696948Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.676183Z",
          "shell.execute_reply": "2023-07-11T11:07:59.695659Z"
        },
        "trusted": true,
        "id": "5L_QSI81ACN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6: How can you find the row for which the value of a specific column is max or min?**\n",
        "\n",
        "`Answer:`"
      ],
      "metadata": {
        "id": "M30kmBftACN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = np.random.randint(1, 10, size=(5, 3))\n",
        "df = pd.DataFrame( data,columns = ['A', 'B', 'C'])\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.702629Z",
          "iopub.execute_input": "2023-07-11T11:07:59.703586Z",
          "iopub.status.idle": "2023-07-11T11:07:59.716494Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.703545Z",
          "shell.execute_reply": "2023-07-11T11:07:59.714997Z"
        },
        "trusted": true,
        "id": "Z8pWYAfYACN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use the pandas idxmax and idxmin function. It's straightforward:\n",
        "\n",
        "Maximal value:"
      ],
      "metadata": {
        "id": "aKhjYmIuACN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.A.idxmax()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.747848Z",
          "iopub.execute_input": "2023-07-11T11:07:59.748229Z",
          "iopub.status.idle": "2023-07-11T11:07:59.755935Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.748198Z",
          "shell.execute_reply": "2023-07-11T11:07:59.754803Z"
        },
        "trusted": true,
        "id": "N5dI-CI-ACN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.B.idxmin()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.758568Z",
          "iopub.execute_input": "2023-07-11T11:07:59.75892Z",
          "iopub.status.idle": "2023-07-11T11:07:59.770425Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.758889Z",
          "shell.execute_reply": "2023-07-11T11:07:59.76925Z"
        },
        "trusted": true,
        "id": "aYv0P81fACN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7: How can you get a list of Pandas DataFrame columns based on data type?**\n",
        "\n",
        "`Answer:`"
      ],
      "metadata": {
        "id": "w4Bnhok7ACN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'NAME': list('abcdef'),\n",
        "        'On_Time': [True, False] * 3,\n",
        "        'On_Budget': [False, True] * 3})\n",
        "print(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.797165Z",
          "iopub.execute_input": "2023-07-11T11:07:59.797573Z",
          "iopub.status.idle": "2023-07-11T11:07:59.807748Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.797539Z",
          "shell.execute_reply": "2023-07-11T11:07:59.806867Z"
        },
        "trusted": true,
        "id": "_-p2HhkpACN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select_dtypes(include = 'boolean')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.809107Z",
          "iopub.execute_input": "2023-07-11T11:07:59.809463Z",
          "iopub.status.idle": "2023-07-11T11:07:59.8263Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.809418Z",
          "shell.execute_reply": "2023-07-11T11:07:59.825506Z"
        },
        "trusted": true,
        "id": "WM2rPch6ACN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = list(df.select_dtypes(include = 'boolean'))\n",
        "my_list"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.827543Z",
          "iopub.execute_input": "2023-07-11T11:07:59.827861Z",
          "iopub.status.idle": "2023-07-11T11:07:59.841686Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.827833Z",
          "shell.execute_reply": "2023-07-11T11:07:59.840638Z"
        },
        "trusted": true,
        "id": "fxbWOaBzACN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8: How does the groupby() method works in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "* In the first stage of the process, data contained in a pandas object, whether a `Series` ,`DataFrame`, or otherwise, is split into groups based on one or more keys that we provide.\n",
        "\n",
        "* The splitting is performed on a particular axis of an object. For example, a **DataFrame** can be grouped on its rows **(axis=0)** or its columns **(axis=1)**.\n",
        "\n",
        "* Once this is done, a function is applied to each group, producing a new value. Finally, the results of all those function applications are combined into a result object. The form of the resulting object will usually depend on what's being done to the data."
      ],
      "metadata": {
        "id": "m51W9qyrACN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9: How to split a string column in a DataFrame into two columns?**\n",
        "\n",
        "`Answer:`\n"
      ],
      "metadata": {
        "id": "GuyK_2XXACN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'row': ['00000 UNITED STATES', '01000 ALABAMA',\n",
        "                           '01001 Autauga County, AL', '01003 Baldwin County, AL',\n",
        "                           '01005 Barbour County, AL']})\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.861653Z",
          "iopub.execute_input": "2023-07-11T11:07:59.862058Z",
          "iopub.status.idle": "2023-07-11T11:07:59.87199Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.862024Z",
          "shell.execute_reply": "2023-07-11T11:07:59.87108Z"
        },
        "trusted": true,
        "id": "yDYLFHXCACN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use `str.split` by whitespace (default separator) and parameter `expand=True` for DataFrame with assign to new columns:"
      ],
      "metadata": {
        "id": "LRLsT6D6ACN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['code', 'location']] = df['row'].str.split(n = 1, expand = True)\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.90061Z",
          "iopub.execute_input": "2023-07-11T11:07:59.900987Z",
          "iopub.status.idle": "2023-07-11T11:07:59.915642Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.900958Z",
          "shell.execute_reply": "2023-07-11T11:07:59.914399Z"
        },
        "trusted": true,
        "id": "098lSiUQACN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10: How to check whether a Pandas DataFrame is empty?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "You can use the attribute `df.empty` to check whether it's empty or not:"
      ],
      "metadata": {
        "id": "W_-LldQlACN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.917752Z",
          "iopub.execute_input": "2023-07-11T11:07:59.918077Z",
          "iopub.status.idle": "2023-07-11T11:07:59.929569Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.91805Z",
          "shell.execute_reply": "2023-07-11T11:07:59.92842Z"
        },
        "trusted": true,
        "id": "4JcScu-kACN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, rows in df.iterrows():\n",
        "    print(rows['c1'], rows['c2'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.931453Z",
          "iopub.execute_input": "2023-07-11T11:07:59.931851Z",
          "iopub.status.idle": "2023-07-11T11:07:59.944099Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.931818Z",
          "shell.execute_reply": "2023-07-11T11:07:59.942921Z"
        },
        "trusted": true,
        "id": "sOXP3wAOACN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q12: What are the operations that Pandas Groupby method is based on ?**\n",
        "\n",
        "`Answer:`\n",
        "* Splitting the data into groups based on some criteria.\n",
        "* Applying a function to each group independently.\n",
        "* Combining the results into a data structure."
      ],
      "metadata": {
        "id": "Myf1YDQfACN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q13: What does describe() percentiles values tell about our data?**\n",
        "\n",
        "`Answer:`\n",
        "The percentiles describe the distribution of your data: **50** should be a value that describes the middle of the data, also known as median. **25**, **75** is the border of the upper/lower quarter of the data. With this can get an idea of how skew our data is."
      ],
      "metadata": {
        "id": "oeslEIhRACN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q14: Why do should make a copy of a DataFrame in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "In general, it is safer to work on copies than on original DataFrames, except when you know that you won't be needing the original anymore and want to proceed with the manipulated version.\n",
        "\n",
        "This is because in Pandas, indexing a DataFrame returns a reference to the initial DataFrame. Thus, changing the subset will change the initial DataFrame. Thus, you'd want to use the copy if you want to make sure the initial DataFrame shouldn't change.\n",
        "\n",
        "Normally, you would still have some use for the original data frame to compare with the manipulated version, etc. Therefore, depending on the case it's a good practice to work on copies and merge at the end."
      ],
      "metadata": {
        "id": "bkBabaIbACN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **15: Compare the Pandas methods: map(), applymap(), apply()**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "1. The **map()** method is an elementwise method for only Pandas Series, it maps values of Series according to input correspondence.\n",
        "\n",
        "    * It accepts dicts, Series, or callable. Values that are not found in the dict are converted to NaN,"
      ],
      "metadata": {
        "id": "DDsi4cQWACN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = pd.Series(['cat', 'dog', np.nan, 'rabbit'])\n",
        "print(s, '\\n')\n",
        "\n",
        "s.map({'cat': 'kitten', 'dog': 'puppy'})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.95439Z",
          "iopub.execute_input": "2023-07-11T11:07:59.955158Z",
          "iopub.status.idle": "2023-07-11T11:07:59.965841Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.955123Z",
          "shell.execute_reply": "2023-07-11T11:07:59.9648Z"
        },
        "trusted": true,
        "id": "qQlO6PLCACN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The **applymap()** method is an elementwise function for only DataFrames, it applies a function that accepts and returns a scalar to every element of a DataFrame.\n",
        "\n",
        "    * It accepts callables only i.e a Python function."
      ],
      "metadata": {
        "id": "F2pgmAS_ACN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([[1, 2.12], [3.356, 4.567]])\n",
        "print(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:07:59.980049Z",
          "iopub.execute_input": "2023-07-11T11:07:59.980469Z",
          "iopub.status.idle": "2023-07-11T11:07:59.98946Z",
          "shell.execute_reply.started": "2023-07-11T11:07:59.980436Z",
          "shell.execute_reply": "2023-07-11T11:07:59.988159Z"
        },
        "trusted": true,
        "id": "zdS9ugEvACN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.applymap(lambda x : int(x *x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.044389Z",
          "iopub.execute_input": "2023-07-11T11:08:00.045464Z",
          "iopub.status.idle": "2023-07-11T11:08:00.056718Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.045426Z",
          "shell.execute_reply": "2023-07-11T11:08:00.055428Z"
        },
        "trusted": true,
        "id": "MZuHupVwACN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The **apply()** method also works elementwise, as it applies a function along input axis of DataFrame. It is suited to more complex operations and aggregation.\n",
        "\n",
        "    * It accepts the callables parameter as well."
      ],
      "metadata": {
        "id": "7T-y6JLJACN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "round(df.apply(np.sqrt),2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.059258Z",
          "iopub.execute_input": "2023-07-11T11:08:00.059957Z",
          "iopub.status.idle": "2023-07-11T11:08:00.075167Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.059915Z",
          "shell.execute_reply": "2023-07-11T11:08:00.073934Z"
        },
        "trusted": true,
        "id": "6pPf5CX8ACN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **16: Describe how you can combine (merge) data on Common Columns or Indices?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "* Using **.merge()** method which merges DataFrame or named Series objects with a database-style join.**You have inner, left, right and outer merge operation**.\n",
        "\n",
        "* By default, the Pandas merge operation acts with an **“inner” merge**. An inner merge, keeps **only the common values in both the left and right dataframes for the result**.\n",
        "\n",
        "* **Left merge**, keeps every row in the left dataframe. Where there are missing values of the “on” variable in the right dataframe, **it adds empty / NaN values in the result**.\n",
        "* **Right merge**, keeps every row in the right dataframe. Where there are missing values of the “on” variable in the left column, **it adds empty / NaN values in the result**.\n",
        "* A full outer join **returns all the rows from the left dataframe**, all the rows from the right dataframe, and matches up rows where possible, with NaNs elsewhere"
      ],
      "metadata": {
        "id": "FW_Hy1-bACN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n",
        "df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.076616Z",
          "iopub.execute_input": "2023-07-11T11:08:00.077642Z",
          "iopub.status.idle": "2023-07-11T11:08:00.088555Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.07761Z",
          "shell.execute_reply": "2023-07-11T11:08:00.087313Z"
        },
        "trusted": true,
        "id": "yDvDxxeqACN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1.merge(df2),'\\n \\n') #By default its Inner merge\n",
        "print(df1.merge(df2, how ='right', on ='a'), '\\n \\n')\n",
        "print(df1.merge(df2, how ='left', on ='a'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.090617Z",
          "iopub.execute_input": "2023-07-11T11:08:00.091176Z",
          "iopub.status.idle": "2023-07-11T11:08:00.126759Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.091142Z",
          "shell.execute_reply": "2023-07-11T11:08:00.125613Z"
        },
        "trusted": true,
        "id": "akJKrWX5ACN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **17.Find a way to binary encode multi-valued categorical variables from a Pandas dataframe**\n",
        "\n",
        "`Answer:`\n",
        "* If [0, 1, 2] are numerical labels and is not the index, then `pandas.DataFrame.pivot_table` works:"
      ],
      "metadata": {
        "id": "B2bYJ0ccACN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame.from_records(\n",
        "    [[0, 'A'], [0, 'B'], [1, 'B'], [1, 'C'], [1, 'D'], [2, 'B'], [2, 'D']],\n",
        "    columns=['number_label', 'category'])\n",
        "data.pivot_table(index=['number_label'], columns=['category'], aggfunc=[len], fill_value=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.128645Z",
          "iopub.execute_input": "2023-07-11T11:08:00.129388Z",
          "iopub.status.idle": "2023-07-11T11:08:00.173929Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.129329Z",
          "shell.execute_reply": "2023-07-11T11:08:00.172656Z"
        },
        "trusted": true,
        "id": "Vs-KRTf8ACN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if [0, 1, 2] is the index, then `collections.Counter` is useful:"
      ],
      "metadata": {
        "id": "rjiRT1XJACN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "data2 = pd.DataFrame.from_dict(\n",
        "    {'categories': {0: ['A', 'B'], 1: ['B', 'C', 'D'], 2:['B', 'D']}})\n",
        "data3 = data2['categories'].apply(collections.Counter)\n",
        "data3 = pd.DataFrame.from_records(data3).fillna(value=0)\n",
        "data3.applymap(lambda x: int(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.175181Z",
          "iopub.execute_input": "2023-07-11T11:08:00.175524Z",
          "iopub.status.idle": "2023-07-11T11:08:00.192808Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.175495Z",
          "shell.execute_reply": "2023-07-11T11:08:00.191419Z"
        },
        "trusted": true,
        "id": "TamoOAoXACN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **18: Group DataFrame Rows into a List**\n",
        "\n",
        "`Answer:`"
      ],
      "metadata": {
        "id": "DMTM5FYoACN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame( {'a':['A','A','B','B','B','C'], 'b':[1,2,5,5,4,6]})\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.195956Z",
          "iopub.execute_input": "2023-07-11T11:08:00.196621Z",
          "iopub.status.idle": "2023-07-11T11:08:00.211041Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.196575Z",
          "shell.execute_reply": "2023-07-11T11:08:00.209845Z"
        },
        "trusted": true,
        "id": "ppS6MMwhACN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The approach here is to use **groupby** to group on the column of interest and then use the `apply()` method to apply list the function to every group generated:\n"
      ],
      "metadata": {
        "id": "8gDaYQelACN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.groupby('a')['b'].apply(list).reset_index(name='new')\n",
        "df1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.212101Z",
          "iopub.execute_input": "2023-07-11T11:08:00.213614Z",
          "iopub.status.idle": "2023-07-11T11:08:00.22871Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.21358Z",
          "shell.execute_reply": "2023-07-11T11:08:00.2274Z"
        },
        "trusted": true,
        "id": "494JWvn8ACN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **19 How can I achieve the equivalents of SQL's IN and NOT IN in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "* Use `pd.Series.isin`.\n",
        "* For **IN** use: **something.isin**(somewhere)"
      ],
      "metadata": {
        "id": "0uv2j1loACOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['a'].isin([3, 6])]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.230436Z",
          "iopub.execute_input": "2023-07-11T11:08:00.231102Z",
          "iopub.status.idle": "2023-07-11T11:08:00.243675Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.231056Z",
          "shell.execute_reply": "2023-07-11T11:08:00.242262Z"
        },
        "trusted": true,
        "id": "G1mNBlKxACOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `NOT IN`: ~**something.isin**(somewhere)"
      ],
      "metadata": {
        "id": "z_e1KGd1ACOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[-df[\"a\"].isin([3, 6])]\n",
        "df[~df[\"a\"].isin([3, 6])]\n",
        "df[df[\"a\"].isin([3, 6]) == False]\n",
        "df[np.logical_not(df[\"a\"].isin([3, 6]))]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.245464Z",
          "iopub.execute_input": "2023-07-11T11:08:00.246177Z",
          "iopub.status.idle": "2023-07-11T11:08:00.263863Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.246078Z",
          "shell.execute_reply": "2023-07-11T11:08:00.262527Z"
        },
        "trusted": true,
        "id": "XM3vGZK0ACOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **20: How do you split a DataFrame according to a boolean criterion?**\n",
        "\n",
        "`Answer:`\n",
        "We can create a mask to separate the dataframe and then use the inverse operator (~) to take the complement of the mask."
      ],
      "metadata": {
        "id": "pLO3PvVZACOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n",
        "                   'B': 'one one two three two two one three'.split(),\n",
        "                   'C': np.arange(8), 'D': np.arange(8) * 2})\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.26801Z",
          "iopub.execute_input": "2023-07-11T11:08:00.268842Z",
          "iopub.status.idle": "2023-07-11T11:08:00.283745Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.268793Z",
          "shell.execute_reply": "2023-07-11T11:08:00.282788Z"
        },
        "trusted": true,
        "id": "z4hCLgIXACOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = df['A'] != 'foo'\n",
        "a, b = df[m], df[~m]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.285282Z",
          "iopub.execute_input": "2023-07-11T11:08:00.285659Z",
          "iopub.status.idle": "2023-07-11T11:08:00.298143Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.28563Z",
          "shell.execute_reply": "2023-07-11T11:08:00.297129Z"
        },
        "trusted": true,
        "id": "Cd7JEbJoACOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a,b)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.299872Z",
          "iopub.execute_input": "2023-07-11T11:08:00.300198Z",
          "iopub.status.idle": "2023-07-11T11:08:00.318481Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.30017Z",
          "shell.execute_reply": "2023-07-11T11:08:00.31762Z"
        },
        "trusted": true,
        "id": "2P0IMFd5ACOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **21: How will you write DataFrame to PostgreSQL table?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "Using Pandas `to_sql` module, you can create an `SQLAlchemy` engine, and write records stored in a DataFrame to a SQL database."
      ],
      "metadata": {
        "id": "DzUvbqaSACOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sqlalchemy import create_engine\n",
        "\n",
        "# # Replace 'your_connection_string' with the actual connection string for your database\n",
        "# connection_string = 'postgresql://username:password@host:port/database_name'\n",
        "\n",
        "# # Create the engine using the valid connection string\n",
        "# engine = create_engine(connection_string)\n",
        "\n",
        "# # Now you can use the engine to write the DataFrame to a table in the database\n",
        "# df.to_sql('table_name', engine)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.319701Z",
          "iopub.execute_input": "2023-07-11T11:08:00.320945Z",
          "iopub.status.idle": "2023-07-11T11:08:00.329852Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.32091Z",
          "shell.execute_reply": "2023-07-11T11:08:00.328618Z"
        },
        "trusted": true,
        "id": "A1M0WCTVACOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **22: How would you encode a large Pandas dataframe using Scikit-Learn?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "We can use **LabelEncoder** to encode a Pandas DataFrame of string or numerical labels. If the dataframe has many columns (`50+`, for example) creating a **LabelEncoder** for each feature is not efficient. In `scikit-learn`, the recommended way is to encode all the features is:"
      ],
      "metadata": {
        "id": "A0hjdjA1ACOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`OneHotEncoder().fit_transform(df)`**"
      ],
      "metadata": {
        "id": "zAH1QHSAACOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **23: How would you convert continuous values into discrete values in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "Depending on the problem, continuous values can be discretized using the `cut()` or `qcut()` function:\n",
        "\n",
        "* `cut()` bins the data based on values. We use it when we need to segment and sort data values into bins evenly spaced. **cut** will choose the bins to be evenly spaced according to the values themselves and not the frequency of those values. For example, **cut** could convert ages to groups of age ranges.\n",
        "\n",
        "* `qcut()` bins the data based on sample quantiles. We use it when we want to have the same number of records in each bin or simply study the data by quantiles. For example, if in a data we have **30** records, and we want to compute the quintiles, **qcut()** will divide the data such that we have **6** records in each bin."
      ],
      "metadata": {
        "id": "9ib_5c_xACOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **24: How would you create Test (20%) and Train (80%) Datasets with Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "scikit learn's `train_test_split` is a good one - it will split both numpy arrays as dataframes."
      ],
      "metadata": {
        "id": "niHY56WnACOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **from sklearn.model_selection import train_test_split**<br>\n",
        "> **train, test = train_test_split(df, test_size=0.2)**"
      ],
      "metadata": {
        "id": "6EjRgYliACOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **25: Is it a good idea to iterate over DataFrame rows in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "Iteration in Pandas is an anti-pattern and is something you should only do when you have exhausted every other option. Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed. You should not use any function with **\"iter\"** in its name for more than a few thousand rows or you will have to get used to a lot of waiting.\n",
        "\n",
        "Do you want to print a DataFrame? Use **DataFrame.to_string()**.\n",
        "\n",
        "Do you want to compute something? In that case, search for methods in this order (list modified from here):\n",
        "\n",
        "* Vectorization\n",
        "* Cython routines\n",
        "* List Comprehensions (vanilla for loop)\n",
        "* **DataFrame.apply():** i)  Reductions that can be performed in Cython, ii) Iteration in Python space\n",
        "* **DataFrame.itertuples()** and **iteritems()**\n",
        "* **DataFrame.iterrows()**\n",
        "\n",
        "`iterrows` and `itertuples` (both receiving many votes in answers to this question) should be used in very rare circumstances, such as generating row objects/nametuples for sequential processing, which is really the only thing these functions are useful for."
      ],
      "metadata": {
        "id": "ZPJ4aYjSACOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **26: Name some type conversion methods in Pandas**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "* `to_numeric()` - provides functionality to safely convert non-numeric types (e.g. strings) to a suitable numeric type.\n",
        "* `astype()` - convert (almost) any type to (almost) any other type. Also allows you to convert to categorial types (very useful).\n",
        "* `infer_objects()` - a utility method to convert object columns holding Python objects to a pandas type if possible. It does this by inferring better dtypes for object columns.\n",
        "* `convert_dtypes()` - convert DataFrame columns to the \"best possible\" `dtype` that supports `pd.NA` (pandas' object to indicate a missing value)."
      ],
      "metadata": {
        "id": "-Hbdg26BACOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **27: Pivot Table Challenge**\n",
        "\n",
        "`Answer:`\n"
      ],
      "metadata": {
        "id": "mHAoWv8dACOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"Col X\": ['class 1', 'class 2', 'class 3', 'class 2'],\n",
        "                   \"Col Y\": ['cat 1', 'cat 1', 'cat 2', 'cat 3']})\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.33193Z",
          "iopub.execute_input": "2023-07-11T11:08:00.332424Z",
          "iopub.status.idle": "2023-07-11T11:08:00.355986Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.332381Z",
          "shell.execute_reply": "2023-07-11T11:08:00.354829Z"
        },
        "trusted": true,
        "id": "vxo97DOgACOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To provide the required **df** we use the function pivot_table with the parameters **index='Col X', columns='Col Y'** and as aggfunc, len:"
      ],
      "metadata": {
        "id": "3fkPs_jSACOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.pivot_table(df, index=['Col X'], columns=['Col Y'], aggfunc=len, fill_value=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.357782Z",
          "iopub.execute_input": "2023-07-11T11:08:00.358135Z",
          "iopub.status.idle": "2023-07-11T11:08:00.380497Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.358105Z",
          "shell.execute_reply": "2023-07-11T11:08:00.379206Z"
        },
        "trusted": true,
        "id": "TAa35Y8hACOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another solution is using groupby on **'Col X','Col Y'** with unstack over **Col Y**, then fill **NaNs** with zeros."
      ],
      "metadata": {
        "id": "oEZNttAIACOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df.groupby((['Col X','Col Y']).size().unstack('Col Y', fill_value=0)['Col Y'], aggfunc=len, fill_value=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.382051Z",
          "iopub.execute_input": "2023-07-11T11:08:00.382463Z",
          "iopub.status.idle": "2023-07-11T11:08:00.388621Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.382429Z",
          "shell.execute_reply": "2023-07-11T11:08:00.387476Z"
        },
        "trusted": true,
        "id": "1xdpEEs8ACOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **29: What is the difference between join() and merge() in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "merge is a function in the pandas namespace, and it is also available as a DataFrame instance method, with the calling DataFrame being implicitly considered the left object in the join.\n",
        "\n",
        "The related DataFrame.join method, uses merge internally for the index-on-index and index-on-column(s) joins, but joins on indexes by default rather than trying to join on common columns (the default behavior for merge). If you are joining on index, you may wish to use DataFrame.join to save yourself some typing.\n",
        "\n",
        "These are the main differences between df.join() and df.merge():\n",
        "\n",
        "* **lookup on right table:** df1.join(df2) always joins via the index of df2, but df1.merge(df2) can join to one or more columns of df2 (default) or to the index of df2 (with right_index=True).\n",
        "* **lookup on left table:** by default, df1.join(df2) uses the index of df1 and df1.merge(df2) uses column(s) of df1. That can be overridden by specifying df1.join(df2, on=key_or_keys) or df1.merge(df2, left_index=True).\n",
        "* **left vs inner join:** df1.join(df2) does a left join by default (keeps all rows of df1), but df.merge does an inner join by default (returns only matching rows of df1 and df2)."
      ],
      "metadata": {
        "id": "CLBoUuPOACOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **30: What is the difference(s) between merge() and concat() in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "At a high level:\n",
        "\n",
        "* concat() simply stacks multiple DataFrame together either vertically, or stitches horizontally after aligning on index\n",
        "* merge() first aligns two DataFrame' selected common column(s) or index, and then pick up the remaining columns from the aligned rows of each DataFrame.\n",
        "\n",
        "More specifically, **.concat():**\n",
        "\n",
        "Is a top-level pandas function\n",
        "Combines two or more pandas `DataFrame` vertically or horizontally\n",
        "Aligns only on the index when combining horizontally\n",
        "Errors when any of the `DataFrame` contains a duplicate index.\n",
        "Defaults to outer join with the option for inner join\n",
        "\n",
        "And **.merge():**\n",
        "\n",
        "* Exists both as a top-level pandas function and a `DataFrame` method (as of pandas 1.0)\n",
        "* Combines exactly two `DataFrame` horizontally\n",
        "* Aligns the calling `DataFrame`'s column(s) or index with the other DataFrame's column(s) or index\n",
        "* Handles duplicate values on the joining columns or index by performing a cartesian product\n",
        "* Defaults to inner join with options for left, outer, and right"
      ],
      "metadata": {
        "id": "2wHh0FVXACOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **31: What's the difference between at and iat in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "`at` and `iat` are functions meant to access a scalar, that is, a single element in the dataframe.\n",
        "\n",
        "With **.at:**\n",
        "\n",
        "* Selection is label based but it only selects a single 'cell' in your DataFrame.\n",
        "* We can assign new indices and columns.\n",
        "* To use .at, pass it both a row and column label separated by a comma.\n",
        "\n",
        "With **.iat:**\n",
        "\n",
        "* Selection with .iat is **position based** but it only selects a single scalar value.\n",
        "* We can't assign new indices and columns.\n",
        "* To use iat you must pass it an integer for both the row and column locations."
      ],
      "metadata": {
        "id": "uarSdOW6ACOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]],\n",
        "     columns=['A', 'B', 'C'])\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.390095Z",
          "iopub.execute_input": "2023-07-11T11:08:00.391096Z",
          "iopub.status.idle": "2023-07-11T11:08:00.407272Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.39105Z",
          "shell.execute_reply": "2023-07-11T11:08:00.406091Z"
        },
        "trusted": true,
        "id": "FDkR0xn9ACOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.at[2,'B',], '\\n\\n')\n",
        "print(df.iat[1, 1])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.411605Z",
          "iopub.execute_input": "2023-07-11T11:08:00.411975Z",
          "iopub.status.idle": "2023-07-11T11:08:00.420527Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.411942Z",
          "shell.execute_reply": "2023-07-11T11:08:00.419292Z"
        },
        "trusted": true,
        "id": "S7XSHupvACOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **32: What's the difference between interpolate() and fillna() in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "* `fillna()` fills the NaN values with a given number with which you want to substitute. It gives you an option to fill according to the index of rows of a pd.DataFrame or on the name of the columns in the form of a python dict.\n",
        "\n",
        "* `interpolate()` it gives you the flexibility to fill the missing values with many kinds of interpolations between the values like linear, time, etc (which fillna does not provide)."
      ],
      "metadata": {
        "id": "bESwnJ8tACOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Q33: What's the difference between pivot_table() and groupby()?**\n",
        "\n",
        "`Answer:`"
      ],
      "metadata": {
        "id": "ruBEzNXPACOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"a\": [1,2,3,1,2,3], \"b\":[1,1,1,2,2,2], \"c\":np.random.rand(6)})\n",
        "pd.pivot_table(df, index=[\"a\"], columns=[\"b\"], values=[\"c\"], aggfunc=np.sum)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.422413Z",
          "iopub.execute_input": "2023-07-11T11:08:00.422856Z",
          "iopub.status.idle": "2023-07-11T11:08:00.453933Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.422815Z",
          "shell.execute_reply": "2023-07-11T11:08:00.452474Z"
        },
        "trusted": true,
        "id": "ccTFB9S9ACOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using **groupby:**\n",
        "\n",
        "* The dimensions given are placed into columns.\n",
        "* The rows are created for each combination of those dimensions.\n",
        "* To obtain the \"equivalent\" output as before, we can create a Series of the sum of values c, grouped by all unique combinations of a and b."
      ],
      "metadata": {
        "id": "tBJhRFFiACOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(['a','b'])['c'].sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:08:00.455253Z",
          "iopub.execute_input": "2023-07-11T11:08:00.455633Z",
          "iopub.status.idle": "2023-07-11T11:08:00.466923Z",
          "shell.execute_reply.started": "2023-07-11T11:08:00.455601Z",
          "shell.execute_reply": "2023-07-11T11:08:00.465741Z"
        },
        "trusted": true,
        "id": "KeO4CCI4ACOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A similar usage of `groupby` is if we omit the ['c']. In this case, it creates a `DataFrame` of the `sums` of all remaining columns grouped by unique values of `a` and `b`."
      ],
      "metadata": {
        "id": "q0_QbX62ACOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby([\"a\",\"b\"]).sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T11:09:53.943575Z",
          "iopub.execute_input": "2023-07-11T11:09:53.943961Z",
          "iopub.status.idle": "2023-07-11T11:09:53.960531Z",
          "shell.execute_reply.started": "2023-07-11T11:09:53.943931Z",
          "shell.execute_reply": "2023-07-11T11:09:53.959216Z"
        },
        "trusted": true,
        "id": "mVV7rnSJACOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see, with pivot_table() the output is a DataFrame, and is easier to specify the changes, meanwhile with groupby() the output can be a DataFrame or Series depending on how we specify the case**"
      ],
      "metadata": {
        "id": "CxJjimOzACOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **34: When to use merge() over concat() and vice-versa in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "* Consider **.concat()** first when combining homogeneous DataFrame, while consider **.merge()** first when combining complementary DataFrame.\n",
        "* If need to merge vertically, go with **.concat()**. If need to merge horizontally via columns, go with .merge(), which by default merge on the columns in common."
      ],
      "metadata": {
        "id": "8HY7SnKMACOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **35: Explain what is Multi-indexing in Pandas?**\n",
        "\n",
        "`Answer:`\n",
        "\n",
        "Multi-indexing, also known as hierarchical indexing, is a feature in pandas that allows you to have multiple levels of row and column labels in a **DataFrame** or Series. It enables you to work with high-dimensional data and organize it in a structured and hierarchical manner.\n",
        "\n",
        "In a multi-indexed DataFrame, each level of the index represents a different category or dimension of the data. This hierarchical structure provides a way to represent and analyze data that has multiple levels of granularity or classification."
      ],
      "metadata": {
        "id": "IGrRcE3eACOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **36: Workers With The Highest Salaries**\n",
        "\n",
        "**Asked In Amazon Interview**\n",
        "\n",
        "You have been asked to find the job titles of the highest-paid employees.\n",
        "Your output should include the highest-paid title or multiple titles with the same salary.\n",
        "\n",
        "![image.png](attachment:e7a00d98-66b0-4301-b5fb-3b08577d7645.png)\n",
        "\n",
        "![image.png](attachment:63650a04-f759-40b6-8e05-d48777c17549.png)\n",
        "\n",
        "\n",
        "**`Answer:`**\n",
        "\n",
        ">import pandas as pd<br>\n",
        "> import numpy as np<br>\n",
        "> title = title.rename(columns = {\"worker_ref_id\":\"worker_id\"})<br>\n",
        "> merged_df = pd.merge(worker, title, on = \"worker_id\")<br>\n",
        "> max_salary = merged_df[merged_df.salary == merged_df.salary.max()][\"worker_title\"]<br>\n",
        "> result = max_salary\n"
      ],
      "metadata": {
        "id": "4ebi5TbLACOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **37: Most Profitable Companies**\n",
        "\n",
        "**Medium: FROBES Interview Question**\n",
        "\n",
        "Find the 3 most profitable companies in the entire world.\n",
        "Output the result along with the corresponding company name.\n",
        "Sort the result based on profits in descending order.\n",
        "\n",
        "![image.png](attachment:1b0d229a-5dd0-403a-909b-fa0c6c303c9f.png)\n",
        "\n",
        "**`Answer:`**\n",
        "\n",
        "> import pandas as pd <br>\n",
        "> forbes_global_2010_2014.head()<br>\n",
        "> forbes_global_2010_2014.sort_values('profits', ascending=False)[['company', 'profits']].iloc[:3]"
      ],
      "metadata": {
        "id": "ivr2XUfQACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **38: Users By Average Session Time**\n",
        "\n",
        "Calculate each user's average session time. A session is defined as the time difference between a page_load and page_exit. For simplicity, assume a user has only 1 session per day and if there are multiple of the same events on that day, consider only the latest page_load and earliest page_exit, with an obvious restriction that load time event should happen before exit time event . Output the user_id and their average session time.\n",
        "\n",
        "![image.png](attachment:d3d2077c-0592-4608-a303-1f48a630133d.png)"
      ],
      "metadata": {
        "id": "ekl4TC_BACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Answer:`\n",
        "\n",
        ">import pandas as pd<br>\n",
        ">facebook_web_log.head()<br>\n",
        ">facebook_web_log.head()<br>\n",
        ">facebook_web_log['date'] = facebook_web_log['timestamp'].dt.date<br>\n",
        ">max_load = facebook_web_log[facebook_web_log['action'] == \"page_load\"].groupby(['user_id','date'])['timestamp'].max()<br>\n",
        ">min_exit = facebook_web_log[facebook_web_log['action'] == \"page_exit\"].groupby(['user_id','date'])['timestamp'].min()<br>\n",
        ">df = (min_exit - max_load).reset_index()<br>\n",
        ">df['timestamp'] = df['timestamp'].dt.seconds<br>\n",
        ">res = df.groupby('user_id')['timestamp'].mean().reset_index()<br>\n",
        ">res.dropna()"
      ],
      "metadata": {
        "id": "_jMjcHKPACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **39: Activity Rank:**\n",
        "\n",
        "Find the email activity rank for each user. Email activity rank is defined by the total number of emails sent. The user with the highest number of emails sent will have a rank of 1, and so on. Output the user, total emails, and their activity rank. Order records by the total emails in descending order. Sort users with the same number of emails in alphabetical order.\n",
        "In your rankings, return a unique value (i.e., a unique rank) even if multiple users have the same number of emails. For tie breaker use alphabetical order of the user usernames.\n",
        "\n",
        "![image.png](attachment:0af50f59-d9b4-4c8f-b7f8-9c4780f00516.png)"
      ],
      "metadata": {
        "id": "by6CEwVaACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Answer:`**\n",
        "\n",
        ">import pandas as pd<br>\n",
        ">import numpy as np<br>\n",
        ">google_gmail_emails.head()<br>\n",
        ">result = google_gmail_emails.groupby(['from_user'], as_index=False).agg(total = ('from_user', 'count'))<br>\n",
        ">result = result.sort_values(['total', 'from_user'], ascending=[False, True])<br>\n",
        ">result['rank'] = result[['total']].rank(ascending=False,method='first')<br>\n",
        "result"
      ],
      "metadata": {
        "id": "ifUmeZQ4ACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **40: Monthly Percentage Difference**\n",
        "\n",
        "Given a table of purchases by date, calculate the month-over-month percentage change in revenue. The output should include the year-month date (YYYY-MM) and percentage change, rounded to the 2nd decimal point, and sorted from the beginning of the year to the end of the year.\n",
        "The percentage change column will be populated from the 2nd month forward and can be calculated as ((this month's revenue - last month's revenue) / last month's revenue)*100.\n",
        "\n",
        "![image.png](attachment:4081fa85-30ee-441a-92bc-f79e5d1a688c.png)"
      ],
      "metadata": {
        "id": "So3usR0OACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Answe:`\n",
        "\n",
        ">import pandas as pd<br>\n",
        ">import numpy as np<br>\n",
        ">from datetime import datetime<br>\n",
        ">pd.options.display.float_format = \"{:,.2f}\".format<br>\n",
        ">sf_transactions['created_at'] = sf_transactions['created_at'].apply(pd.to_datetime)<br>\n",
        ">sf_transactions['year_month'] = pd.to_datetime(sf_transactions['created_at']).dt.to_period('M')<br>\n",
        ">df = sf_transactions.groupby('year_month')<br>['value'].sum().reset_index(name='monthly_revenue').sort_values('year_month')<br>\n",
        ">df['prev_value'] = df['monthly_revenue'].shift(1)<br>\n",
        ">df['revenue_diff_pct'] = round(((df['monthly_revenue'] - df['prev_value'])/df['prev_value'])*100, 2)<br>\n",
        "result = df[['year_month','revenue_diff_pct']].fillna('')\n",
        "\n",
        "\n",
        "* First create a new column names `year_month` which will extract month and year only from created_at column.\n",
        "* Group by the new column `year_month` now and sum the values column tab to see the total values used month wise w.r.t to the year.\n",
        "*"
      ],
      "metadata": {
        "id": "R2LRewgCACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SQL**\n",
        "\n",
        "**Patients** table consists of columns and data like this below:\n",
        "![image.png](attachment:49279e80-76b3-45d6-9539-1b9ea24bee77.png)"
      ],
      "metadata": {
        "id": "ShQONrQGACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Easy 1: Show first name, last name, and gender of patients whose gender is 'M'**"
      ],
      "metadata": {
        "id": "irxV9joaACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        ">SELECT first_name,<br>\n",
        "       last_name, <br>\n",
        "       gender <br>\n",
        "FROM patients<br>\n",
        "WHERE gender = 'M'"
      ],
      "metadata": {
        "id": "gmfBi1yRACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Easy 2: Show first name and last name of patients who does not have allergies. (null)**"
      ],
      "metadata": {
        "id": "Jo0OAamaACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        ">SELECT<br>\n",
        "\tfirst_name,<br>\n",
        "    last_name<br>\n",
        "FROM patients<br>\n",
        "WHERE allergies != 'NULL'"
      ],
      "metadata": {
        "id": "-bBqeoNiACOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Easy 3: Show first name and last name of patients that weight within the range of 100 to 120 (inclusive)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        ">select first_name,<br>\n",
        "\t   last_name<br>\n",
        "from patients<br>\n",
        "where weight between 100 and 120"
      ],
      "metadata": {
        "id": "ZCaDKejgACOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Easy 4:Show first name and last name concatinated into one column to show their full name.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "\n",
        ">select concat(first_name,\" \", last_name) as full_name<br>\n",
        "from patients"
      ],
      "metadata": {
        "id": "nefcTsBtACOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Easy 5: Show first name, last name, and the full province name of each patient.**\n",
        "\n",
        "Example: 'Ontario' instead of 'ON'\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> SELECT<br>\n",
        "  first_name,<br>\n",
        "  last_name,<br>\n",
        "  province_name<br>\n",
        "FROM patients<br>\n",
        "JOIN province_names ON province_names.province_id = patients.province_id;"
      ],
      "metadata": {
        "id": "KIOlzDi3ACOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Easy 6: Show how many patients have a birth_date with 2010 as the birth year.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        ">SELECT<br>\n",
        "  count(patient_id)<br>\n",
        "  from patients<br>\n",
        "  where birth_date LIKE \"2010%\"<br>"
      ],
      "metadata": {
        "id": "c3VJGW11ACOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 7: Show patient_id and first_name from patients where their first_name start and ends with 's' and is at least 6 characters long.**\n",
        "\n",
        "**Answer:**\n",
        "> select patient_id, first_name<br>\n",
        "from patients<br>\n",
        "where first_name like \"s%s\"<br>\n",
        "AND lEN(first_name) >=6"
      ],
      "metadata": {
        "id": "JvXBrvCjACOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 8: Show unique birth years from patients and order them by ascending.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        ">SELECT DISTINCT year (birth_date) <br>\n",
        "FROM patients<br>\n",
        "ORDER BY birth_date ASC;"
      ],
      "metadata": {
        "id": "w-5u60QIACOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 9: Show the first_name, last_name, and height of the patient with the greatest height.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> select first_name, last_name, max(height) AS height<br>\n",
        "from patients\n"
      ],
      "metadata": {
        "id": "fGOEYGhmACOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 10: Show unique first names from the patients table which only occurs once in the list.**\n",
        "\n",
        "For example, if two or more people are named 'John' in the first_name column then don't include their name in the output list. If only 1 person is named 'Leo' then include them in the output.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> SELECT first_name<br>\n",
        "FROM patients<br>\n",
        "GROUP BY first_name<br>\n",
        "HAVING COUNT(first_name) = 1;"
      ],
      "metadata": {
        "id": "hYNLYl3mACOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 11: Show patient_id, first_name, last_name from patients whos diagnosis is 'Dementia'.**\n",
        "\n",
        "Primary diagnosis is stored in the admissions table.\n",
        "\n",
        "**Answer:**\n",
        "> SELECT<br>\n",
        "  patients.patient_id,<br>\n",
        "  first_name,<br>\n",
        "  last_name<br>\n",
        "FROM patients<br>\n",
        "JOIN admissions ON admissions.patient_id = patients.patient_id<br>\n",
        "WHERE diagnosis = 'Dementia';\n"
      ],
      "metadata": {
        "id": "bUT2sIS6ACOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 12: Display every patient's first_name. Order the list by the length of each name and then by alphbetically**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> SELECT<br>\n",
        "  first_name<br>\n",
        "from patients<br>\n",
        "order by len(first_name), first_name asc"
      ],
      "metadata": {
        "id": "XJuqlCvWACOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 13: Show the total amount of male patients and the total amount of female patients in the patients table.Display the two results in the same row.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> SELECT <br>\n",
        "  (SELECT count(*) FROM patients WHERE gender='M') AS male_count, <br>\n",
        "  (SELECT count(*) FROM patients WHERE gender='F') AS female_count;"
      ],
      "metadata": {
        "id": "isa_YHEdACOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 14: Show first and last name, allergies from patients which have allergies to either 'Penicillin' or 'Morphine'. Show results ordered ascending by allergies then by first_name then by last_name.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> select first_name, last_name, allergies<br>\n",
        "from patients<br>\n",
        "where allergies = \"Penicillin\" or allergies = \"Morphine\"<br>\n",
        "order by allergies, first_name, last_name"
      ],
      "metadata": {
        "id": "rKJwCaJAACOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 15: Show patient_id, diagnosis from admissions. Find patients admitted multiple times for the same diagnosis.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> select patient_id,diagnosis<br>\n",
        "from admissions<br>\n",
        "group by patient_id, diagnosis<br>\n",
        "having count(*)>1"
      ],
      "metadata": {
        "id": "fXHCYAIoACOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 16: Show the city and the total number of patients in the city.Order from most to least patients and then by city name ascending.**\n",
        "\n",
        "**Answer:**\n",
        "> select city, count(patient_id) as num_patients<br>\n",
        "from patients<br>\n",
        "group by city<br>\n",
        "order by num_patients desc, city asc;"
      ],
      "metadata": {
        "id": "dzvELYobACOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 17: Show first name, last name and role of every person that is either patient or doctor.The roles are either \"Patient\" or \"Doctor\"**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> select city, count(patient_id) as num_patients<br>\n",
        "from patients<br>\n",
        "group by city<br>\n",
        "order by num_patients desc, city asc;"
      ],
      "metadata": {
        "id": "rM0RlrfQACOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 18: name, last name and role of every person that is either patient or doctor.The roles are either \"Patient\" or \"Doctor\"**\n",
        "\n",
        "**Answer:**\n",
        ">SELECT first_name, last_name, 'Patient' AS role<br>\n",
        "FROM patients<br>\n",
        "UNION ALL<br>\n",
        "SELECT first_name, last_name, 'Doctor' AS role<br>\n",
        "FROM doctors;"
      ],
      "metadata": {
        "id": "u784J4PBACOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 19: Show all allergies ordered by popularity. Remove NULL values from query.**\n",
        "\n",
        "**Answer:**\n",
        "> SELECT allergies, COUNT(*) AS popularity<br>\n",
        "FROM patients<br>\n",
        "WHERE allergies IS NOT NULL<br>\n",
        "GROUP BY allergies<br>\n",
        "ORDER BY popularity DESC;\n"
      ],
      "metadata": {
        "id": "TdIXGjWEACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 20: Show all patient's first_name, last_name, and birth_date who were born in the 1970s decade. Sort the list starting from the earliest birth_date.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> SELECT<br>\n",
        "  first_name,<br>\n",
        "  last_name,<br>\n",
        "  birth_date<br>\n",
        "FROM patients<br>\n",
        "WHERE YEAR(birth_date) BETWEEN 1970 AND 1979<br>\n",
        "ORDER BY birth_date ASC"
      ],
      "metadata": {
        "id": "QE4QQpWFACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 21: We want to display each patient's full name in a single column. Their last_name in all upper letters must appear first, then first_name in all lower case letters. Separate the last_name and first_name with a comma. Order the list by the first_name in decending order**\n",
        "EX: SMITH,jane\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> SELECT CONCAT(UPPER(last_name), ',', LOWER(first_name)) AS full_name<br>\n",
        "FROM patients<br>\n",
        "ORDER BY first_name DESC;"
      ],
      "metadata": {
        "id": "Y7OMh7YkACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 22: Show the province_id(s), sum of height; where the total sum of its patient's height is greater than or equal to 7,000.**\n",
        "\n",
        "**Answer:**\n",
        "> SELECT province_id, SUM(height) AS total_height<br>\n",
        "FROM patients<br>\n",
        "GROUP BY province_id<br>\n",
        "HAVING SUM(height) >= 7000;\n"
      ],
      "metadata": {
        "id": "FGpnacpAACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 23: Show the difference between the largest weight and smallest weight for patients with the last name 'Maroni'**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> select max(weight) - MIN(weight) as Diff <br>\n",
        "from patients<br>\n",
        "where last_name = \"Maroni\""
      ],
      "metadata": {
        "id": "Zn9PDWl9ACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 24: Show all of the days of the month (1-31) and how many admission_dates occurred on that day. Sort by the day with most admissions to least admissions.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        ">SELECT DAY (admission_date) AS day_of_month,<br>\n",
        "\t   COUNT(*) AS admissions_count<br>\n",
        "FROM admissions<br>\n",
        "GROUP BY day_of_month<br>\n",
        "ORDER BY admissions_count DESC;\n"
      ],
      "metadata": {
        "id": "ovHveOs3ACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 25: Show all columns for patient_id 542's most recent admission_date.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        ">SELECT *<br>\n",
        "FROM admissions<br>\n",
        "WHERE patient_id = 542<br>\n",
        "GROUP BY patient_id<br>\n",
        "HAVING<br>\n",
        "  admission_date = MAX(admission_date);"
      ],
      "metadata": {
        "id": "l8Gx51UOACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 26: Show patient_id, attending_doctor_id, and diagnosis for admissions that match one of the two criteria:**\n",
        "1. patient_id is an odd number and attending_doctor_id is either 1, 5, or 19.\n",
        "2. attending_doctor_id contains a 2 and the length of patient_id is 3 characters.\n",
        "\n",
        "**Answer:**\n",
        "> SELECT patient_id, attending_doctor_id, diagnosis<br>\n",
        "FROM admissions<br>\n",
        "WHERE <br>\n",
        "  (patient_id % 2 = 1 AND attending_doctor_id IN (1, 5, 19))<br>\n",
        "  OR<br>\n",
        "  (attending_doctor_id LIKE '%2%' AND LENGTH(patient_id) = 3);<br>\n"
      ],
      "metadata": {
        "id": "GZSMLcKfACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 27: Show first_name, last_name, and the total number of admissions attended for each doctor.Every admission has been attended by a doctor.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> select d.first_name,d.last_name, count(*) AS total_admission<br>\n",
        "from doctors d<br>\n",
        "JOIN<br>\n",
        "admissions a ON d.doctor_id = a.attending_doctor_id<br>\n",
        "group by d.first_name, d.last_name\n"
      ],
      "metadata": {
        "id": "zjL7LQpaACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 28: For each doctor, display their id, full name, and the first and last admission date they attended.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        ">select d.doctor_id, concat(first_name, ' ', last_name) as full_name,<br>\n",
        "\t   Min(a.admission_date) as first_admission_date,<br>\n",
        "       MAX(a.admission_date) as last_admission_date<br>\n",
        "       from doctors d\n",
        "JOIn admissions a <br>\n",
        "ON d.doctor_id = a.attending_doctor_id<br>\n",
        "GROUP BY d.doctor_id, full_name"
      ],
      "metadata": {
        "id": "ipuaqX5fACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 29: Display the total amount of patients for each province. Order by descending.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> select p.province_name, count(*) AS patient_count<br>\n",
        "from province_names p <br>\n",
        "JOIn patients <br>\n",
        "ON p.province_id = patients.province_id<br>\n",
        "group by province_name<br>\n",
        "order by patient_count desc"
      ],
      "metadata": {
        "id": "Bs9Z0391ACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 30:For every admission, display the patient's full name, their admission diagnosis, and their doctor's full name who diagnosed their problem.**\n",
        "\n",
        "**Answer:**\n",
        "> SELECT<br>\n",
        "  CONCAT(patients.first_name, ' ', patients.last_name) as patient_name,<br>\n",
        "  diagnosis,<br>\n",
        "  CONCAT(doctors.first_name,' ',doctors.last_name) as doctor_name<br>\n",
        "FROM patients<br>\n",
        "  JOIN admissions ON admissions.patient_id = patients.patient_id<br>\n",
        "  JOIN doctors ON doctors.doctor_id = admissions.attending_doctor_id;"
      ],
      "metadata": {
        "id": "YsTWfkvOACOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 31: display the number of duplicate patients based on their first_name and last_name.**\n",
        "\n",
        "**Answer:**\n",
        ">  select first_name,last_name, count(*) as no_of_duplicates<br>\n",
        "  from patients<br>\n",
        "  group by first_name,last_name<br>\n",
        "  having count(*)>1"
      ],
      "metadata": {
        "id": "VW08lMxuACOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 32: Display patient's full name,height in the units feet rounded to 1 decimal, weight in the unit pounds rounded to 0 decimals birth_date,gender non abbreviated.Convert CM to feet by dividing by 30.48.Convert KG to pounds by multiplying by 2.205.**\n",
        "\n",
        "\n",
        "**Answer:**\n",
        "\n",
        ">SELECT<br>\n",
        "    CONCAT(first_name, ' ', last_name) AS full_name,<br>\n",
        "    ROUND(height / 30.48, 1) AS height_feet,<br>\n",
        "    ROUND(weight * 2.205, 0) AS weight_pounds,<br>\n",
        "    birth_date,<br>\n",
        "    CASE<br>\n",
        "        WHEN gender = 'M' THEN 'Male'<br>\n",
        "        WHEN gender = 'F' THEN 'Female'<br>\n",
        "        ELSE 'Other'<br>\n",
        "    END AS gender_non_abbreviated<br>\n",
        "FROM<br>\n",
        "    patients;\n"
      ],
      "metadata": {
        "id": "3WcKBGoQACOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Medium 33: Show patient_id, first_name, last_name from patients whose does not have any records in the admissions table. (Their patient_id does not exist in any admissions.patient_id rows.)**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "> SELECT patient_id, first_name, last_name<br>\n",
        "FROM patients<br>\n",
        "WHERE patient_id NOT IN (SELECT DISTINCT patient_id FROM admissions);"
      ],
      "metadata": {
        "id": "mzzHW2U9ACOK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jKrMUDD0ACOK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}